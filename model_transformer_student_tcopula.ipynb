{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1KHViH_yXF0",
        "outputId": "43451476-ca0d-4f54-d820-b806566edff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaDiTSo4ygHw",
        "outputId": "364bc0d7-5d6b-4ccc-8a64-dc62d31850f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ROBUST_TIME SERIES_CLASSIFICATION/AnomalyBERT-main\n"
          ]
        }
      ],
      "source": [
        "%cd gdrive/MyDrive/ROBUST_TIME SERIES_CLASSIFICATION/AnomalyBERT-main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0upJUhczCUl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Training loop\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import MultivariateNormal\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_curve, auc\n",
        "from torch.nn.utils.rnn import pad_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9YhicYRXTfX",
        "outputId": "55fc5562-b403-4321-8268-e5d70d3801a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(93873, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data_norm_anom = pd.read_csv('/content/gdrive/MyDrive/DATASETS/TIME_SERIES_MULTI_V_DATA/SWAT_NORMALIZED.csv')\n",
        "data_norm_anom = data_norm_anom.iloc[0:100000]\n",
        "data_norm_anom = data_norm_anom.dropna()\n",
        "\n",
        "df_normal = data_norm_anom[data_norm_anom['Label'] == 0]\n",
        "df_normal = df_normal.iloc[:, 0:-1]\n",
        "df_normal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XlZrhcYWz5zq"
      },
      "outputs": [],
      "source": [
        "data_norm_anom = pd.read_csv('/content/gdrive/MyDrive/DATASETS/TIME_SERIES_MULTI_V_DATA/SWAT_NORMALIZED.csv')\n",
        "data_norm_anom = data_norm_anom.iloc[0:20000]\n",
        "data_norm_anom = data_norm_anom.dropna()\n",
        "\n",
        "df_anomaly = data_norm_anom[data_norm_anom['Label'] == 1]\n",
        "df_anomaly = df_anomaly.iloc[:, 0:-1]\n",
        "df_anomaly.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ppx0spNZ6y_T"
      },
      "outputs": [],
      "source": [
        "df_anomaly = df_anomaly.values\n",
        "df_anomaly.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb879izWg0aO",
        "outputId": "9bcab52d-d9f6-4df3-9d16-d38e9fc38be0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(93873, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_normal = df_normal.values\n",
        "df_normal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tz7oVl5LU2cz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD-Ze8_Ch0nk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class SyntheticAnomalyGenerator:\n",
        "    def __init__(self,\n",
        "                 window_size,\n",
        "                 reference_anomalies,\n",
        "                 anomaly_types=['local_perturbation']):\n",
        "        \"\"\"\n",
        "        Initializes the anomaly generator that oversamples real anomaly data\n",
        "        but applies small perturbations to create a local neighborhood around\n",
        "        the real anomalies.\n",
        "\n",
        "        Args:\n",
        "            window_size (int): The size of the anomaly window to inject.\n",
        "            reference_anomalies (np.ndarray): Real anomaly data from which to oversample.\n",
        "              Shape could be (num_anomalies, sequence_length, num_features) or\n",
        "              (num_anomalies, num_features) if each row is a snippet, etc.\n",
        "            anomaly_types (list): We keep it flexible, but default to ['local_perturbation'].\n",
        "        \"\"\"\n",
        "        self.anomaly_types = anomaly_types\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Real anomaly data to oversample from\n",
        "        self.reference_anomalies = reference_anomalies\n",
        "\n",
        "    def inject_anomaly_into_sequence(self, sequence):\n",
        "        \"\"\"\n",
        "        Called externally to inject an anomaly into 'sequence' by\n",
        "        picking a real anomaly snippet, perturbing it, and overlaying.\n",
        "        \"\"\"\n",
        "        anomalous_sequence = sequence.copy()\n",
        "        anomaly_type = random.choice(self.anomaly_types)\n",
        "\n",
        "        seq_length, num_features = anomalous_sequence.shape\n",
        "\n",
        "        if seq_length <= self.window_size:\n",
        "            t0 = 0\n",
        "            t1 = seq_length\n",
        "        else:\n",
        "            t0 = random.randint(0, seq_length - self.window_size)\n",
        "            t1 = t0 + self.window_size\n",
        "\n",
        "        if anomaly_type == 'local_perturbation':\n",
        "            anomalous_sequence = self.local_perturbation_injection(anomalous_sequence, t0, t1)\n",
        "        elif anomaly_type == 'permutation_anomaly':\n",
        "            # optional, if you want partial permutations\n",
        "            anomalous_sequence = self.permutation_anomaly(anomalous_sequence, t0, t1)\n",
        "\n",
        "        return anomalous_sequence\n",
        "\n",
        "    def local_perturbation_injection(self, sequence, t0, t1):\n",
        "        \"\"\"\n",
        "        1) Randomly pick a real anomaly snippet from reference_anomalies.\n",
        "        2) Possibly adapt or clamp it to 'window_size'.\n",
        "        3) Add small random perturbations (noise, slight scaling, partial permutations, etc.).\n",
        "        4) Overlay onto 'sequence[t0:t1]'.\n",
        "        \"\"\"\n",
        "        anomalous_sequence = sequence.copy()\n",
        "        window_length = t1 - t0\n",
        "        seq_length, num_features = anomalous_sequence.shape\n",
        "\n",
        "        # 1) pick a random anomaly snippet from reference_anomalies\n",
        "        #    Suppose reference_anomalies has shape (num_anomalies, anomaly_length, num_features)\n",
        "        #    or (num_anomalies, num_features) if each row is a snippet.\n",
        "        rand_idx = random.randint(0, self.reference_anomalies.shape[0]-1)\n",
        "        real_anomaly_snippet = self.reference_anomalies[rand_idx]\n",
        "\n",
        "        # Check if real_anomaly_snippet is 2D or 1D\n",
        "        if real_anomaly_snippet.ndim == 1:\n",
        "            # shape (num_features,) => we replicate or clamp it\n",
        "            # e.g. we'll just tile it for 'window_length'\n",
        "            real_anomaly_snippet = np.tile(real_anomaly_snippet, (window_length,1))\n",
        "        elif real_anomaly_snippet.shape[0] != window_length:\n",
        "            # We do a clamp or pad approach:\n",
        "            real_len = real_anomaly_snippet.shape[0]\n",
        "            if real_len > window_length:\n",
        "                # clamp\n",
        "                start_ = random.randint(0, real_len - window_length)\n",
        "                real_anomaly_snippet = real_anomaly_snippet[start_:start_+window_length, :]\n",
        "            else:\n",
        "                # if real_len < window_length => pad repeats or edge\n",
        "                repeats_needed = window_length - real_len\n",
        "                # simplest => tile or pad\n",
        "                real_anomaly_snippet = np.concatenate([\n",
        "                    real_anomaly_snippet,\n",
        "                    np.tile(real_anomaly_snippet[-1,:], (repeats_needed,1))\n",
        "                ], axis=0)\n",
        "\n",
        "        # Now real_anomaly_snippet is shape (window_length, num_features)\n",
        "\n",
        "        # 2) Add small perturbations\n",
        "        perturbed_snippet = self.apply_small_perturbations(real_anomaly_snippet)\n",
        "\n",
        "        # 3) Overlay\n",
        "        anomalous_sequence[t0:t1] = perturbed_snippet\n",
        "        return anomalous_sequence\n",
        "\n",
        "    def apply_small_perturbations(self, snippet):\n",
        "        \"\"\"\n",
        "        snippet shape: (window_length, num_features)\n",
        "        We'll do e.g.:\n",
        "          - small gaussian noise\n",
        "          - slight random scaling per feature\n",
        "          - partial permutations of a few rows (rare)\n",
        "        all to keep changes mild.\n",
        "        \"\"\"\n",
        "        w_len, n_feats = snippet.shape\n",
        "        snippet_ = snippet.copy()\n",
        "\n",
        "        # A) small gaussian noise\n",
        "        noise_std = 0.05 * np.std(snippet_)  # e.g. 5% of snippet's std\n",
        "        noise = np.random.normal(0, noise_std, snippet_.shape)\n",
        "        snippet_ += noise\n",
        "\n",
        "        # B) slight random scaling per feature\n",
        "        # e.g. each feature f => scale_f in [0.95,1.05]\n",
        "        scales = 0.95 + 0.1*np.random.rand(n_feats)\n",
        "        for f in range(n_feats):\n",
        "            snippet_[:, f] *= scales[f]\n",
        "\n",
        "        # C) partial row permutation, only a few rows\n",
        "        # e.g. we permute 10% of the snippet rows:\n",
        "        row_count = int(0.1 * w_len)  # 10% of rows\n",
        "        if row_count>1:\n",
        "            perm_indices = np.random.choice(w_len, size=row_count, replace=False)\n",
        "            # shuffle those rows\n",
        "            snippet_[perm_indices] = snippet_[np.random.permutation(perm_indices)]\n",
        "\n",
        "        return snippet_\n",
        "\n",
        "    def permutation_anomaly(self, sequence, t0, t1):\n",
        "        \"\"\"\n",
        "        If you want a direct 'permutation_anomaly' approach:\n",
        "        \"\"\"\n",
        "        anomalous_sequence = sequence.copy()\n",
        "        window = anomalous_sequence[t0:t1]\n",
        "        perm_indices = np.random.permutation(len(window))\n",
        "        permuted_window = window[perm_indices]\n",
        "        anomalous_sequence[t0:t1] = permuted_window\n",
        "        return anomalous_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uzF-BNuJA_j"
      },
      "outputs": [],
      "source": [
        "def extract_sequences(time_series_data, window_size, step_size):\n",
        "    total_length, num_features = time_series_data.shape\n",
        "    sequences = []\n",
        "\n",
        "    for start_idx in range(0, total_length - window_size + 1, step_size):\n",
        "        end_idx = start_idx + window_size\n",
        "        sequence = time_series_data[start_idx:end_idx]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39G_oCPWJKYv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxIDWZgjH24Y"
      },
      "outputs": [],
      "source": [
        "def generate_consecutive_pairs(sequences, sequence_labels):\n",
        "    \"\"\"\n",
        "    Generate pairs of consecutive sequences.\n",
        "\n",
        "    Args:\n",
        "        sequences (list): List of sequences.\n",
        "        sequence_labels (list): Corresponding labels for each sequence.\n",
        "\n",
        "    Returns:\n",
        "        x1_pairs (list): List of sequences for x1.\n",
        "        x2_pairs (list): List of sequences for x2.\n",
        "        pair_labels (list): List of labels for each pair (0 for normal-normal, 1 for anomaly-involved).\n",
        "    \"\"\"\n",
        "    x1_pairs = []\n",
        "    x2_pairs = []\n",
        "    pair_labels = []\n",
        "\n",
        "    for i in range(len(sequences) - 1):\n",
        "        x1 = sequences[i]\n",
        "        x2 = sequences[i + 1]\n",
        "        label = 0  # Default to normal-normal\n",
        "\n",
        "        # If either sequence is anomalous, label the pair as anomaly-involved\n",
        "        if sequence_labels[i] == 1 or sequence_labels[i + 1] == 1:\n",
        "            label = 1\n",
        "\n",
        "        x1_pairs.append(x1)\n",
        "        x2_pairs.append(x2)\n",
        "        pair_labels.append(label)\n",
        "\n",
        "    return x1_pairs, x2_pairs, pair_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2NEep7tJOON"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    x1_batch, x2_batch, labels = zip(*batch)\n",
        "\n",
        "    x1_batch = [torch.tensor(x, dtype=torch.float32) for x in x1_batch]\n",
        "    x2_batch = [torch.tensor(x, dtype=torch.float32) for x in x2_batch]\n",
        "\n",
        "    x1_lengths = torch.tensor([seq.size(0) for seq in x1_batch], dtype=torch.long)\n",
        "    x2_lengths = torch.tensor([seq.size(0) for seq in x2_batch], dtype=torch.long)\n",
        "\n",
        "    padded_x1 = pad_sequence(x1_batch, batch_first=True, padding_value=0.0)\n",
        "    padded_x2 = pad_sequence(x2_batch, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)  # For contrastive loss, labels should be float\n",
        "\n",
        "    max_len_x1 = padded_x1.size(1)\n",
        "    max_len_x2 = padded_x2.size(1)\n",
        "\n",
        "    range_x1 = torch.arange(max_len_x1).unsqueeze(0).expand(len(x1_lengths), max_len_x1)\n",
        "    range_x2 = torch.arange(max_len_x2).unsqueeze(0).expand(len(x2_lengths), max_len_x2)\n",
        "\n",
        "    x1_mask = (range_x1 >= x1_lengths.unsqueeze(1)).to(torch.bool)\n",
        "    x2_mask = (range_x2 >= x2_lengths.unsqueeze(1)).to(torch.bool)\n",
        "\n",
        "    return padded_x1, padded_x2, labels, x1_mask, x2_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CH8PPt7YFaRd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH1jWcXSIs_p"
      },
      "outputs": [],
      "source": [
        "def apply_synthetic_anomalies_to_sequences(sequences, anomaly_percentage, anomaly_generator):\n",
        "    \"\"\"\n",
        "    Inject synthetic anomalies into a specified percentage of sequences.\n",
        "\n",
        "    Args:\n",
        "        sequences (list): List of sequences extracted from the time series data.\n",
        "        anomaly_percentage (float): Percentage of sequences to inject anomalies (0.1 to 1.0).\n",
        "        anomaly_generator (SyntheticAnomalyGenerator): An instance of the anomaly generator.\n",
        "\n",
        "    Returns:\n",
        "        augmented_sequences (list): List of sequences after injecting anomalies.\n",
        "        sequence_labels (list): Labels for each sequence (0 for normal, 1 for anomaly).\n",
        "    \"\"\"\n",
        "    num_sequences = len(sequences)\n",
        "    num_anomalous_sequences = int(num_sequences * anomaly_percentage)\n",
        "    anomaly_indices = set(random.sample(range(num_sequences), num_anomalous_sequences))\n",
        "\n",
        "    augmented_sequences = []\n",
        "    sequence_labels = []\n",
        "\n",
        "    for i, seq in enumerate(sequences):\n",
        "        if i in anomaly_indices:\n",
        "            # Inject anomaly into the entire sequence\n",
        "            augmented_seq = anomaly_generator.inject_anomaly_into_sequence(seq)\n",
        "            augmented_sequences.append(augmented_seq)\n",
        "            sequence_labels.append(1)\n",
        "        else:\n",
        "            # Normal sequence\n",
        "            augmented_sequences.append(seq)\n",
        "            sequence_labels.append(0)\n",
        "\n",
        "    return augmented_sequences, sequence_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2tSjWGtFdOm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n-bHfX-Fhp6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x1_pairs, x2_pairs, pair_labels, transform=None):\n",
        "        assert len(x1_pairs) == len(x2_pairs) == len(pair_labels), \"All inputs must have the same length.\"\n",
        "        self.x1_pairs = x1_pairs\n",
        "        self.x2_pairs = x2_pairs\n",
        "        self.pair_labels = pair_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pair_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x1 = self.x1_pairs[idx]\n",
        "        x2 = self.x2_pairs[idx]\n",
        "        label = self.pair_labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            x1 = self.transform(x1)\n",
        "            x2 = self.transform(x2)\n",
        "\n",
        "        return x1, x2, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4Cu5msgJT8Z"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer, num_layers=num_layers\n",
        "        )\n",
        "        self.output_proj = nn.Linear(model_dim, model_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        x = self.input_proj(x)  # (batch_size, seq_length, model_dim)\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)  # (batch_size, seq_length, model_dim)\n",
        "\n",
        "        if src_key_padding_mask is not None:\n",
        "            # Create data mask: True where data is present\n",
        "            data_mask = (~src_key_padding_mask).unsqueeze(2).type_as(x)  # (batch_size, seq_length, 1)\n",
        "            x = x * data_mask  # Zero out padding positions: (batch_size, seq_length, model_dim)\n",
        "            x_sum = x.sum(dim=1)  # Sum over seq_length: (batch_size, model_dim)\n",
        "\n",
        "            lengths = data_mask.sum(dim=1)  # Sum over seq_length: (batch_size, 1)\n",
        "            lengths = lengths.clamp(min=1)  # Avoid division by zero\n",
        "            x = x_sum / lengths  # (batch_size, model_dim)\n",
        "        else:\n",
        "            x = x.mean(dim=1)  # Average over seq_length: (batch_size, model_dim)\n",
        "\n",
        "        x = self.output_proj(x)  # (batch_size, model_dim)\n",
        "        x = self.activation(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VvQDw3GJ7pc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.special import gammaln\n",
        "\n",
        "def _multivariate_student_t_logpdf(x, L, nu):\n",
        "    \"\"\"\n",
        "    Computes the log density of a multivariate Student-t distribution at point x.\n",
        "\n",
        "    x:  (batch_size, d)\n",
        "    L:  (d, d) lower-triangular Cholesky factor of correlation matrix Sigma => Sigma = L L^T\n",
        "    nu: scalar degrees of freedom\n",
        "\n",
        "    Returns: (batch_size,) log density under T_{nu, Sigma}.\n",
        "    \"\"\"\n",
        "    eps = 1e-12\n",
        "    batch_size, d = x.shape\n",
        "\n",
        "    # 1) precision = (L L^T)^{-1} => we can do cholesky_inverse(L)\n",
        "    #    sigma = L @ L^T, so sigma^{-1} = (L^{-1})^T (L^{-1})\n",
        "    Sigma_inv = torch.cholesky_inverse(L)\n",
        "\n",
        "    # 2) log|Sigma| => 2 * sum(log(diag(L)))\n",
        "    log_det_Sigma = 2.0 * torch.log(torch.diagonal(L, 0)).sum()\n",
        "\n",
        "    # 3) Quadratic form => x^T Sigma^{-1} x\n",
        "    quadform = torch.einsum('bi,ij,bj->b', x, Sigma_inv, x)  # shape (batch_size,)\n",
        "\n",
        "    # 4) Student-t logpdf formula:\n",
        "    #    log f_{t_{nu,Sigma}}(x)\n",
        "    #  =  Gamma((nu+d)/2) - Gamma(nu/2)\n",
        "    #     - 0.5 log|Sigma|\n",
        "    #     - d/2 log(nu pi)\n",
        "    #     - (nu+d)/2 log(1 + (1/nu)*x^T Sigma^{-1} x)\n",
        "    d = float(d)\n",
        "    nu_ = nu + eps  # ensure positivity\n",
        "    c1 = (gammaln(0.5*(nu_ + d)) - gammaln(0.5*nu_))\n",
        "    c2 = -0.5*log_det_Sigma\n",
        "    c3 = -0.5*d*(math.log(math.pi*nu_+eps))\n",
        "\n",
        "    # final\n",
        "    logprob = c1 + c2 + c3 - 0.5*(nu_ + d)*torch.log1p(quadform/(nu_+eps))\n",
        "    return logprob\n",
        "\n",
        "class StudentTCopula_Normal(nn.Module):\n",
        "    \"\"\"\n",
        "    A Student-t Copula capturing heavy-tailed dependence in the latent space.\n",
        "    Similar logic to the GaussianCopula, but we do:\n",
        "      1) standardize z\n",
        "      2) transform z => U in (0,1) via Tcdf\n",
        "      3) transform U => z_trans via Ticdf\n",
        "      4) build correlation matrix from L (cholesky factor)\n",
        "      5) Evaluate multivariate T log-likelihood\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super(StudentTCopula_Normal, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Cholesky factor for correlation\n",
        "        n_tril_elements = (latent_dim * (latent_dim + 1)) // 2\n",
        "        self.L_params = nn.Parameter(torch.randn(n_tril_elements)*0.1)\n",
        "\n",
        "        # degrees of freedom param\n",
        "        # we do softplus for positivity\n",
        "        self.nu_param = nn.Parameter(torch.tensor(5.0))\n",
        "\n",
        "    def forward(self, z, mu=None, std=None):\n",
        "        \"\"\"\n",
        "        z: (batch_size, latent_dim)\n",
        "        mu, std: optional mean/std for standardizing z. If None, we compute from z.\n",
        "        returns: (batch_size,) log-likelihood under the Student-t copula\n",
        "        \"\"\"\n",
        "        device = z.device\n",
        "        eps = 1e-6\n",
        "        batch_size, d = z.shape\n",
        "\n",
        "        # 1) standardize z\n",
        "        if mu is None or std is None:\n",
        "            mu_ = z.mean(dim=0, keepdim=True)\n",
        "            std_ = z.std(dim=0, keepdim=True, unbiased=False)+eps\n",
        "        else:\n",
        "            mu_ = mu.to(device)\n",
        "            std_ = std.to(device)\n",
        "            # ensure positivity\n",
        "            std_ = torch.where(std_>0, std_, eps*torch.ones_like(std_))\n",
        "\n",
        "        z_standard = (z - mu_) / std_\n",
        "        z_standard = torch.clamp(z_standard, min=-10.0, max=10.0)\n",
        "\n",
        "        # 2) transform to U in (0,1) via Student T cdf\n",
        "        # PyTorch doesn't have direct Tcdf or Ticdf, so we might rely on SciPy or custom approach\n",
        "        # For demonstration, let's do a small function that calls scipy t.cdf or t.ppf\n",
        "        # We'll define a helper below or approximate. We'll do a \"detach().cpu().numpy()\" approach.\n",
        "\n",
        "        u = _student_t_cdf(z_standard, self._nu_value().item())\n",
        "        u = torch.clamp(u, min=eps, max=1.0 - eps)\n",
        "\n",
        "        # 3) transform U => z_trans via T icdf\n",
        "        z_trans = _student_t_ppf(u, self._nu_value().item())\n",
        "\n",
        "        # 4) build correlation from L_params\n",
        "        L = torch.zeros(d, d, device=device)\n",
        "        tril_indices = torch.tril_indices(row=d, col=d, offset=0)\n",
        "        L[tril_indices[0], tril_indices[1]] = self.L_params\n",
        "\n",
        "        # ensure positive diagonal\n",
        "        diag_idx = torch.arange(d, device=device)\n",
        "        diag_vals = torch.diagonal(L).clone()\n",
        "        diag_vals = F.softplus(diag_vals) + eps\n",
        "        L[diag_idx, diag_idx] = diag_vals\n",
        "\n",
        "        # 5) Evaluate multivariate T logpdf\n",
        "        nu_ = self._nu_value()\n",
        "        log_prob = _multivariate_student_t_logpdf(z_trans, L, nu_)\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "    def _nu_value(self):\n",
        "        \"\"\"\n",
        "        Return positive degrees of freedom.\n",
        "        We add a small eps to ensure positivity.\n",
        "        \"\"\"\n",
        "        return F.softplus(self.nu_param)+1e-4\n",
        "\n",
        "def _student_t_cdf(z, nu):\n",
        "    \"\"\"\n",
        "    Compute univariate student-t cdf for each entry in z, do it elementwise.\n",
        "    We'll call SciPy if available. Otherwise, a placeholder approach.\n",
        "    Because we need z => shape (batch, dim).\n",
        "    We'll do a detach to CPU, compute via scipy, then back to torch.\n",
        "    This breaks gradient, but for typical copula-based anomaly detection,\n",
        "    we only backprop through correlation params, not the entire cdf.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.stats import t as scipy_t\n",
        "\n",
        "    z_np = z.detach().cpu().numpy()\n",
        "    u_np = scipy_t.cdf(z_np, df=nu)\n",
        "    return torch.tensor(u_np, device=z.device, dtype=z.dtype)\n",
        "\n",
        "def _student_t_ppf(u, nu):\n",
        "    \"\"\"\n",
        "    Inverse of the cdf for Student-t (ppf).\n",
        "    We'll do the same approach with SciPy.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.stats import t as scipy_t\n",
        "\n",
        "    u_np = u.detach().cpu().numpy()\n",
        "    z_np = scipy_t.ppf(u_np, df=nu)\n",
        "    return torch.tensor(z_np, device=u.device, dtype=u.dtype)\n"
      ],
      "metadata": {
        "id": "d_Inco-q7Beg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.special import gammaln\n",
        "\n",
        "def _multivariate_student_t_logpdf(x, L, nu):\n",
        "    \"\"\"\n",
        "    Computes the log density of a multivariate Student-t distribution at point x.\n",
        "\n",
        "    x:  (batch_size, d)\n",
        "    L:  (d, d) lower-triangular Cholesky factor of correlation matrix Sigma => Sigma = L L^T\n",
        "    nu: scalar degrees of freedom\n",
        "\n",
        "    Returns: (batch_size,) log density under T_{nu, Sigma}.\n",
        "    \"\"\"\n",
        "    eps = 1e-12\n",
        "    batch_size, d = x.shape\n",
        "\n",
        "    # 1) precision = (L L^T)^{-1} => we can do cholesky_inverse(L)\n",
        "    #    sigma = L @ L^T, so sigma^{-1} = (L^{-1})^T (L^{-1})\n",
        "    Sigma_inv = torch.cholesky_inverse(L)\n",
        "\n",
        "    # 2) log|Sigma| => 2 * sum(log(diag(L)))\n",
        "    log_det_Sigma = 2.0 * torch.log(torch.diagonal(L, 0)).sum()\n",
        "\n",
        "    # 3) Quadratic form => x^T Sigma^{-1} x\n",
        "    quadform = torch.einsum('bi,ij,bj->b', x, Sigma_inv, x)  # shape (batch_size,)\n",
        "\n",
        "    # 4) Student-t logpdf formula:\n",
        "    #    log f_{t_{nu,Sigma}}(x)\n",
        "    #  =  Gamma((nu+d)/2) - Gamma(nu/2)\n",
        "    #     - 0.5 log|Sigma|\n",
        "    #     - d/2 log(nu pi)\n",
        "    #     - (nu+d)/2 log(1 + (1/nu)*x^T Sigma^{-1} x)\n",
        "    d = float(d)\n",
        "    nu_ = nu + eps  # ensure positivity\n",
        "    c1 = (gammaln(0.5*(nu_ + d)) - gammaln(0.5*nu_))\n",
        "    c2 = -0.5*log_det_Sigma\n",
        "    c3 = -0.5*d*(math.log(math.pi*nu_+eps))\n",
        "\n",
        "    # final\n",
        "    logprob = c1 + c2 + c3 - 0.5*(nu_ + d)*torch.log1p(quadform/(nu_+eps))\n",
        "    return logprob\n",
        "\n",
        "class StudentTCopula_Anomaly(nn.Module):\n",
        "    \"\"\"\n",
        "    A Student-t Copula capturing heavy-tailed dependence in the latent space.\n",
        "    Similar logic to the GaussianCopula, but we do:\n",
        "      1) standardize z\n",
        "      2) transform z => U in (0,1) via Tcdf\n",
        "      3) transform U => z_trans via Ticdf\n",
        "      4) build correlation matrix from L (cholesky factor)\n",
        "      5) Evaluate multivariate T log-likelihood\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super(StudentTCopula_Anomaly, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Cholesky factor for correlation\n",
        "        n_tril_elements = (latent_dim * (latent_dim + 1)) // 2\n",
        "        self.L_params = nn.Parameter(torch.randn(n_tril_elements)*0.1)\n",
        "\n",
        "        # degrees of freedom param\n",
        "        # we do softplus for positivity\n",
        "        self.nu_param = nn.Parameter(torch.tensor(5.0))\n",
        "\n",
        "    def forward(self, z, mu=None, std=None):\n",
        "        \"\"\"\n",
        "        z: (batch_size, latent_dim)\n",
        "        mu, std: optional mean/std for standardizing z. If None, we compute from z.\n",
        "        returns: (batch_size,) log-likelihood under the Student-t copula\n",
        "        \"\"\"\n",
        "        device = z.device\n",
        "        eps = 1e-6\n",
        "        batch_size, d = z.shape\n",
        "\n",
        "        # 1) standardize z\n",
        "        if mu is None or std is None:\n",
        "            mu_ = z.mean(dim=0, keepdim=True)\n",
        "            std_ = z.std(dim=0, keepdim=True, unbiased=False)+eps\n",
        "        else:\n",
        "            mu_ = mu.to(device)\n",
        "            std_ = std.to(device)\n",
        "            # ensure positivity\n",
        "            std_ = torch.where(std_>0, std_, eps*torch.ones_like(std_))\n",
        "\n",
        "        z_standard = (z - mu_) / std_\n",
        "        z_standard = torch.clamp(z_standard, min=-10.0, max=10.0)\n",
        "\n",
        "        # 2) transform to U in (0,1) via Student T cdf\n",
        "        # PyTorch doesn't have direct Tcdf or Ticdf, so we might rely on SciPy or custom approach\n",
        "        # For demonstration, let's do a small function that calls scipy t.cdf or t.ppf\n",
        "        # We'll define a helper below or approximate. We'll do a \"detach().cpu().numpy()\" approach.\n",
        "\n",
        "        u = _student_t_cdf(z_standard, self._nu_value().item())\n",
        "        u = torch.clamp(u, min=eps, max=1.0 - eps)\n",
        "\n",
        "        # 3) transform U => z_trans via T icdf\n",
        "        z_trans = _student_t_ppf(u, self._nu_value().item())\n",
        "\n",
        "        # 4) build correlation from L_params\n",
        "        L = torch.zeros(d, d, device=device)\n",
        "        tril_indices = torch.tril_indices(row=d, col=d, offset=0)\n",
        "        L[tril_indices[0], tril_indices[1]] = self.L_params\n",
        "\n",
        "        # ensure positive diagonal\n",
        "        diag_idx = torch.arange(d, device=device)\n",
        "        diag_vals = torch.diagonal(L).clone()\n",
        "        diag_vals = F.softplus(diag_vals) + eps\n",
        "        L[diag_idx, diag_idx] = diag_vals\n",
        "\n",
        "        # 5) Evaluate multivariate T logpdf\n",
        "        nu_ = self._nu_value()\n",
        "        log_prob = _multivariate_student_t_logpdf(z_trans, L, nu_)\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "    def _nu_value(self):\n",
        "        \"\"\"\n",
        "        Return positive degrees of freedom.\n",
        "        We add a small eps to ensure positivity.\n",
        "        \"\"\"\n",
        "        return F.softplus(self.nu_param)+1e-4\n",
        "\n",
        "def _student_t_cdf(z, nu):\n",
        "    \"\"\"\n",
        "    Compute univariate student-t cdf for each entry in z, do it elementwise.\n",
        "    We'll call SciPy if available. Otherwise, a placeholder approach.\n",
        "    Because we need z => shape (batch, dim).\n",
        "    We'll do a detach to CPU, compute via scipy, then back to torch.\n",
        "    This breaks gradient, but for typical copula-based anomaly detection,\n",
        "    we only backprop through correlation params, not the entire cdf.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.stats import t as scipy_t\n",
        "\n",
        "    z_np = z.detach().cpu().numpy()\n",
        "    u_np = scipy_t.cdf(z_np, df=nu)\n",
        "    return torch.tensor(u_np, device=z.device, dtype=z.dtype)\n",
        "\n",
        "def _student_t_ppf(u, nu):\n",
        "    \"\"\"\n",
        "    Inverse of the cdf for Student-t (ppf).\n",
        "    We'll do the same approach with SciPy.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy.stats import t as scipy_t\n",
        "\n",
        "    u_np = u.detach().cpu().numpy()\n",
        "    z_np = scipy_t.ppf(u_np, df=nu)\n",
        "    return torch.tensor(z_np, device=u.device, dtype=u.dtype)\n"
      ],
      "metadata": {
        "id": "0j3OwMT77BhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VrJHGhYg7BkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt-ZGHdUgKjM"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def contrastive_loss(ll_gaussian, ll_vine, pair_labels, alpha=0.5, base_margin= 0.0):\n",
        "    \"\"\"\n",
        "      A contrastive loss for separating normal vs. anomaly log-likelihoods with a *dynamic* margin:\n",
        "\n",
        "      1) Normal frames => maximize log-likelihood => loss = -LL_normal\n",
        "      2) Anomaly frames => LL_anomaly <= (mean_normal_LL - base_offset) on a per-sample basis.\n",
        "\n",
        "    Now we receive both Gaussian and Studen-t LL. We pick the appropriate LL based on the labels:\n",
        "    - Normal pairs (label=0) -> Gaussian Copula LL\n",
        "    - Anomaly pairs (label=1) -> Vine Copula LL\n",
        "    \"\"\"\n",
        "    pair_labels = pair_labels.view(-1)\n",
        "    # Select appropriate LL based on labels\n",
        "    chosen_ll = torch.where(pair_labels == 0, ll_gaussian, ll_vine)\n",
        "\n",
        "    normal_ll = chosen_ll[pair_labels == 0]\n",
        "    anomaly_ll = chosen_ll[pair_labels == 1]\n",
        "    #print (f\"Normal LL: {normal_ll.mean()}\")\n",
        "    #print (f\"Anomaly LL: {anomaly_ll.mean()}\")\n",
        "\n",
        "    if len(normal_ll) > 0:\n",
        "        loss_normal = -normal_ll\n",
        "        mean_normal_ll = normal_ll.mean()\n",
        "    else:\n",
        "        loss_normal = torch.tensor(0.0, device=chosen_ll.device)\n",
        "        mean_normal_ll = torch.tensor(0.0, device=chosen_ll.device)\n",
        "\n",
        "    # -----------------------\n",
        "    # Anomaly portion\n",
        "    # -----------------------\n",
        "    # We'll define a *dynamic margin* for anomalies\n",
        "    # margin_dyn = mean_normal_ll - base_offset\n",
        "    # Then for each anomaly sample 'a', we want a <= margin_dyn => diff = a - margin_dyn => F.relu(diff)\n",
        "    # If diff>0 => penalty\n",
        "    if len(anomaly_ll) > 0:\n",
        "        margin_dyn = mean_normal_ll - base_margin\n",
        "        # shape (#anomalies,)\n",
        "        diff = anomaly_ll - margin_dyn\n",
        "        # sample-level margin\n",
        "        loss_anomaly = alpha * F.relu(diff).mean()\n",
        "    else:\n",
        "        loss_anomaly = torch.tensor(0.0, device=device)\n",
        "\n",
        "    total_loss = (1 - alpha) * loss_normal.mean() + loss_anomaly.mean()\n",
        "\n",
        "\n",
        "      # Monitoring: Print average log-likelihoods for debugging\n",
        "    if len(normal_ll) > 0:\n",
        "        print(f\"Contrastive Loss - Normal LL Mean: {loss_normal.mean().item():.4f}\")\n",
        "    else:\n",
        "        print(\"Contrastive Loss - No Normal Samples in Batch.\")\n",
        "\n",
        "    if len(anomaly_ll) > 0:\n",
        "        print(f\"Contrastive Loss - Anomalous LL Mean: {loss_anomaly.mean().item():.4f}\")\n",
        "    else:\n",
        "        print(\"Contrastive Loss - No Anomalous Samples in Batch.\")\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Ne1Xtq2Znk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFDlWwpCNLA7"
      },
      "outputs": [],
      "source": [
        "class AnomalyTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout, embedding_dim=64):\n",
        "        super(AnomalyTransformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder(input_dim, model_dim, num_heads, num_layers, dropout)\n",
        "\n",
        "        # We now have two different copulas\n",
        "        self.gaussian_copula = StudentTCopula_Normal(latent_dim=2 * model_dim)\n",
        "        self.vine_copula = StudentTCopula_Anomaly(latent_dim=2 * model_dim)\n",
        "\n",
        "        self.embedding_layer = nn.Linear(2 * model_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x1, x2, mask1=None, mask2=None):\n",
        "        z1 = self.encoder(x1, src_key_padding_mask=mask1)\n",
        "        z2 = self.encoder(x2, src_key_padding_mask=mask2)\n",
        "        z = torch.cat((z1, z2), dim=1)\n",
        "\n",
        "        # Compute both Gaussian and Vine likelihoods\n",
        "        ll_gaussian = self.gaussian_copula(z)\n",
        "        ll_vine = self.vine_copula(z)\n",
        "        return ll_gaussian, ll_vine\n",
        "\n",
        "    def get_embeddings(self, x1, x2, mask1=None, mask2=None):\n",
        "        # Instead of passing x1, x2 as (B, features),\n",
        "        # we do x1 => (B, 1, features)\n",
        "        if x1.dim() == 2:\n",
        "            x1 = x1.unsqueeze(1)  # shape => (B,1,features)\n",
        "        if x2.dim() == 2:\n",
        "            x2 = x2.unsqueeze(1)\n",
        "\n",
        "        transformed_x1 = self.encoder(x1, src_key_padding_mask=mask1)\n",
        "        transformed_x2 = self.encoder(x2, src_key_padding_mask=mask2)\n",
        "\n",
        "        # Now each is likely shape (B, seq_len=1, model_dim) or (B, model_dim)\n",
        "        # if the encoder does a mean over seq_len => shape (B, model_dim).\n",
        "        # We need to confirm the final shape. If the encoder returns 3D => e.g. (B,1,model_dim),\n",
        "        # we might want to reduce it to 2D => (B,model_dim) by mean over dim=1.\n",
        "        if transformed_x1.dim() == 3:\n",
        "            # e.g. if shape => (B,1,hidden_dim), reduce to (B, hidden_dim)\n",
        "            transformed_x1 = transformed_x1.mean(dim=1)\n",
        "        if transformed_x2.dim() == 3:\n",
        "            transformed_x2 = transformed_x2.mean(dim=1)\n",
        "\n",
        "        # Now each => (B, hidden_dim)\n",
        "        combined = torch.cat((transformed_x1, transformed_x2), dim=1)  # => (B,2*hidden_dim)\n",
        "        embeddings = self.embedding_layer(combined)  # => (B, embed_dim)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_kb7pmqI45z"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-SCQwEaGSAt",
        "outputId": "7048f7e4-603b-418c-a560-ea08418b7770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total extracted sequences: 1876\n",
            "Total anomalous sequences: 1313\n",
            "Total generated pairs: 1875\n",
            "Anomaly-involved pairs: 1695\n",
            "Normal-normal pairs: 180\n"
          ]
        }
      ],
      "source": [
        "window_size = 100\n",
        "step_size = 50  # Non-overlapping sequences\n",
        "anomaly_percentage = 0.7 # 20% of sequences will be anomalous\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# Step 1: Extract sequences from the raw data\n",
        "sequences = extract_sequences(df_normal, window_size=window_size, step_size=step_size)\n",
        "print(f\"Total extracted sequences: {len(sequences)}\")\n",
        "\n",
        "\n",
        "anomaly_generator = SyntheticAnomalyGenerator(\n",
        "    window_size=50,\n",
        "    reference_anomalies=df_anomaly,\n",
        "    anomaly_types=['local_perturbation']  # can add 'permutation_anomaly' if desired\n",
        ")\n",
        "\n",
        "# Step 2: Inject anomalies into sequences\n",
        "augmented_sequences, sequence_labels = apply_synthetic_anomalies_to_sequences(\n",
        "    sequences,\n",
        "    anomaly_percentage=anomaly_percentage,\n",
        "    anomaly_generator=anomaly_generator\n",
        ")\n",
        "print(f\"Total anomalous sequences: {sum(sequence_labels)}\")\n",
        "\n",
        "# Step 3: Generate consecutive pairs\n",
        "x1_pairs, x2_pairs, pair_labels = generate_consecutive_pairs(augmented_sequences, sequence_labels)\n",
        "print(f\"Total generated pairs: {len(pair_labels)}\")\n",
        "print(f\"Anomaly-involved pairs: {sum(pair_labels)}\")\n",
        "print(f\"Normal-normal pairs: {len(pair_labels) - sum(pair_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjZNUIy0Wm-t",
        "outputId": "e8cc4406-28e1-4dd5-ed14-96929e7883a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch\n",
        "\n",
        "# 1) Train dataset from x1_pairs, x2_pairs, pair_labels\n",
        "train_dataset = TimeSeriesDataset(\n",
        "    x1_pairs=x1_pairs,\n",
        "    x2_pairs=x2_pairs,\n",
        "    pair_labels=pair_labels,\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUvKnT3YCsB7",
        "outputId": "7b184a0f-24c2-41d0-ab0c-f0d5ecaed824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 26) (100000,)\n"
          ]
        }
      ],
      "source": [
        "# 2) Build a new dataset for the separate test data\n",
        "import pandas as pd\n",
        "data_norm_anom = pd.read_csv('/content/gdrive/MyDrive/DATASETS/TIME_SERIES_MULTI_V_DATA/SWAT_NORMALIZED.csv')\n",
        "data_norm_anom = data_norm_anom.dropna()\n",
        "\n",
        "data_test = data_norm_anom.iloc[100000:200000]\n",
        "test_features = data_test.iloc[:, :-1].values\n",
        "test_labels   = data_test.iloc[:,  -1].values\n",
        "print (test_features.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls3iG0MMC0vR"
      },
      "outputs": [],
      "source": [
        "# Define a simple dataset\n",
        "class SimpleTimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, transform=None):\n",
        "        self.features = features\n",
        "        self.labels   = labels\n",
        "        self.transform= transform\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]\n",
        "        y = self.labels[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "test_dataset_all = SimpleTimeSeriesDataset(test_features, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3zyapjZC0zA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRj_TRdpC6n2",
        "outputId": "0ce6c1c8-b263-4cae-b4dc-44801792daae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 3) Split that dataset into val + test\n",
        "test_dataset_size = len(test_dataset_all)\n",
        "val_size = test_dataset_size // 2\n",
        "val_indices = list(range(val_size))\n",
        "test_indices= list(range(val_size, test_dataset_size))\n",
        "\n",
        "val_subset  = torch.utils.data.Subset(test_dataset_all, val_indices)\n",
        "test_subset = torch.utils.data.Subset(test_dataset_all, test_indices)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_subset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef8026EHKgWT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C76R97uCGWEd"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_dim = df_normal.shape[1]  # Number of features in the input data\n",
        "model_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "model = AnomalyTransformer(input_dim=input_dim, model_dim=model_dim, num_heads=num_heads, num_layers=num_layers, dropout=dropout).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZwU47uWq4bL"
      },
      "source": [
        "**PLOTTING AND STATS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTLF2VXNq_by"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B9hGtHBr_Qf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Function to calculate F1_PA\n",
        "def calculate_f1_pa(labels, preds):\n",
        "    true_segments = []\n",
        "    pred_segments = []\n",
        "\n",
        "    # Get segments for true labels\n",
        "    i = 0\n",
        "    while i < len(labels):\n",
        "        if labels[i] == 1:\n",
        "            start = i\n",
        "            while i < len(labels) and labels[i] == 1:\n",
        "                i += 1\n",
        "            true_segments.append((start, i - 1))\n",
        "        i += 1\n",
        "\n",
        "    # Get segments for predictions\n",
        "    i = 0\n",
        "    while i < len(preds):\n",
        "        if preds[i] == 1:\n",
        "            start = i\n",
        "            while i < len(preds) and preds[i] == 1:\n",
        "                i += 1\n",
        "            pred_segments.append((start, i - 1))\n",
        "        i += 1\n",
        "\n",
        "    # Compute overlap\n",
        "    tp = 0\n",
        "    for t_start, t_end in true_segments:\n",
        "        for p_start, p_end in pred_segments:\n",
        "            if not (p_end < t_start or p_start > t_end):\n",
        "                tp += 1\n",
        "                break\n",
        "\n",
        "    precision_pa = tp / len(pred_segments) if len(pred_segments) > 0 else 0\n",
        "    recall_pa = tp / len(true_segments) if len(true_segments) > 0 else 0\n",
        "    f1_pa = 2 * precision_pa * recall_pa / (precision_pa + recall_pa) if precision_pa + recall_pa > 0 else 0\n",
        "\n",
        "    return f1_pa\n",
        "\n",
        "\n",
        "# Validation Function\n",
        "def validate(model, val_loader, device, threshold=None):\n",
        "    model.eval()\n",
        "    all_log_likelihoods = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Check how many items are in the batch\n",
        "            if len(batch) == 5:\n",
        "                # The original multi-input scenario\n",
        "                padded_x1, padded_x2, labels, x1_mask, x2_mask = batch\n",
        "                padded_x1 = padded_x1.to(device, dtype=torch.float32)\n",
        "                padded_x2 = padded_x2.to(device, dtype=torch.float32)\n",
        "                labels = labels.to(device)\n",
        "                x1_mask = x1_mask.to(device)\n",
        "                x2_mask = x2_mask.to(device)\n",
        "\n",
        "                ll_gaussian, ll_vine = model(padded_x1, padded_x2, mask1=x1_mask, mask2=x2_mask)\n",
        "\n",
        "            if len(batch) == 3:\n",
        "                padded_x1, padded_x2, labels = batch\n",
        "                padded_x1 = padded_x1.to(device, dtype=torch.float32)\n",
        "                padded_x2 = padded_x2.to(device, dtype=torch.float32)\n",
        "                labels    = labels.to(device)\n",
        "\n",
        "                ll_gaussian, ll_vine = model(padded_x1, padded_x2)\n",
        "\n",
        "            elif len(batch) == 2:\n",
        "                features, labels = batch\n",
        "                features = features.to(device, dtype=torch.float32)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Unsqueeze so shape => (B, 1, num_features)\n",
        "                features = features.unsqueeze(1)\n",
        "\n",
        "                ll_gaussian, ll_vine = model(features, features)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected number of items in batch: {len(batch)}\")\n",
        "\n",
        "            # Now we have (ll_gaussian, ll_vine) and 'labels'\n",
        "            # Select final LL:  normal => Gaussian, anomaly => Vine\n",
        "            ll_final = torch.where(labels == 0, ll_gaussian, ll_vine)\n",
        "\n",
        "            all_log_likelihoods.extend(ll_final.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert to np.array\n",
        "    all_log_likelihoods = np.array(all_log_likelihoods)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # If no threshold provided, find best by F1\n",
        "    if threshold is None:\n",
        "        best_f1 = 0.0\n",
        "        best_threshold = 0.0\n",
        "        mins, maxs = all_log_likelihoods.min(), all_log_likelihoods.max()\n",
        "        for thresh in np.linspace(mins, maxs, 100):\n",
        "            preds = (all_log_likelihoods < thresh).astype(int)\n",
        "            f1 = f1_score(all_labels, preds, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = thresh\n",
        "        threshold = best_threshold\n",
        "\n",
        "    # Generate final predictions using threshold\n",
        "    preds = (all_log_likelihoods < threshold).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, preds)\n",
        "    precision = precision_score(all_labels, preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, preds, zero_division=0)\n",
        "    auc_roc = roc_auc_score(all_labels, -all_log_likelihoods)  # negative => lower LL => anomaly\n",
        "\n",
        "    # Calculate F1_PA\n",
        "    f1_pa = calculate_f1_pa(all_labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'auc_roc': auc_roc,\n",
        "        'f1_pa': f1_pa,\n",
        "        'threshold': threshold\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z937tz292aC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7X-6dCVD7fO"
      },
      "outputs": [],
      "source": [
        "def plot_log_likelihoods(model, loader, device, dataset_name=\"Dataset\"):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from torch import no_grad\n",
        "\n",
        "    model.eval()\n",
        "    log_likelihoods = []\n",
        "    labels = []\n",
        "\n",
        "    normal_log_likelihoods = []\n",
        "    anomaly_log_likelihoods = []\n",
        "\n",
        "    with no_grad():\n",
        "        for batch in loader:\n",
        "            if len(batch) == 5:\n",
        "                # old approach: (padded_x1, padded_x2, batch_labels, x1_mask, x2_mask)\n",
        "                padded_x1, padded_x2, batch_labels, x1_mask, x2_mask = batch\n",
        "\n",
        "                padded_x1 = padded_x1.to(device)\n",
        "                padded_x2 = padded_x2.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                x1_mask = x1_mask.to(device)\n",
        "                x2_mask = x2_mask.to(device)\n",
        "\n",
        "                # model returns (ll_gaussian, ll_vine)\n",
        "                ll_gaussian, ll_vine = model(padded_x1, padded_x2, mask1=x1_mask, mask2=x2_mask)\n",
        "\n",
        "            elif len(batch) == 2:\n",
        "                # new approach: (features, labels)\n",
        "                features, batch_labels = batch\n",
        "                # shape => (batch_size, features) or (batch_size, seq_len, features)\n",
        "                features = features.to(device, dtype=torch.float32)\n",
        "                features = features.unsqueeze(1)  # => shape (B,1,features_dim)\n",
        "\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                # If your model expects two inputs, we pass the same feature twice\n",
        "                ll_gaussian, ll_vine = model(features, features)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected number of items in batch: {len(batch)}\")\n",
        "\n",
        "            # Now we have ll_gaussian, ll_vine, and batch_labels\n",
        "            # Normal => label=0 => use ll_gaussian\n",
        "            # Anomaly=> label=1 => use ll_vine\n",
        "            selected_ll = torch.where(batch_labels == 0, ll_gaussian, ll_vine)\n",
        "\n",
        "            # Convert to CPU/NumPy\n",
        "            selected_ll_np = selected_ll.cpu().numpy()\n",
        "            labels_np = batch_labels.cpu().numpy()\n",
        "\n",
        "            # Separate normal vs. anomaly for debugging\n",
        "            normal_mask = (labels_np == 0)\n",
        "            anomaly_mask= (labels_np == 1)\n",
        "\n",
        "            normal_ll  = selected_ll_np[normal_mask]\n",
        "            anomaly_ll = selected_ll_np[anomaly_mask]\n",
        "\n",
        "            if len(normal_ll) > 0:\n",
        "                normal_log_likelihoods.extend(normal_ll)\n",
        "            if len(anomaly_ll) > 0:\n",
        "                anomaly_log_likelihoods.extend(anomaly_ll)\n",
        "\n",
        "            log_likelihoods.extend(selected_ll_np)\n",
        "            labels.extend(labels_np)\n",
        "\n",
        "    log_likelihoods = np.array(log_likelihoods)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Build a DataFrame for plotting\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\n",
        "        'Log-Likelihood': log_likelihoods,\n",
        "        'Label': ['Anomaly' if lb == 1 else 'Normal' for lb in labels]\n",
        "    })\n",
        "\n",
        "    # Summary stats\n",
        "    if len(normal_log_likelihoods) > 0:\n",
        "        mean_normal = np.mean(normal_log_likelihoods)\n",
        "    else:\n",
        "        mean_normal = float('nan')\n",
        "\n",
        "    if len(anomaly_log_likelihoods) > 0:\n",
        "        mean_anomaly = np.mean(anomaly_log_likelihoods)\n",
        "    else:\n",
        "        mean_anomaly = float('nan')\n",
        "\n",
        "    print(f\"[{dataset_name}] Mean Normal Log-Likelihood: {mean_normal:.4f}\")\n",
        "    print(f\"[{dataset_name}] Mean Anomaly Log-Likelihood: {mean_anomaly:.4f}\")\n",
        "\n",
        "    # Plot distributions via Seaborn\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(data=df, x='Log-Likelihood', hue='Label', fill=True, common_norm=False, alpha=0.5)\n",
        "    plt.title(f'Log-Likelihood Distribution for {dataset_name} Set')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmRolkPqZB4i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "num_epochs = 25\n",
        "epoch_losses = []\n",
        "val_metrics_history = []\n",
        "\n",
        "# Arrays to store metrics each epoch\n",
        "auc_roc_values   = []\n",
        "accuracy_values  = []\n",
        "precision_values = []\n",
        "recall_values    = []\n",
        "\n",
        "# (If needed) define your LR scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "best_f1           = 0.0\n",
        "patience_counter  = 0\n",
        "patience          = 10\n",
        "\n",
        "try:\n",
        "    for epoch in range(num_epochs):\n",
        "        ################################################\n",
        "        # 1) TRAIN\n",
        "        ################################################\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (padded_x1, padded_x2, labels, x1_mask, x2_mask) in enumerate(train_loader):\n",
        "            padded_x1 = padded_x1.to(device)\n",
        "            padded_x2 = padded_x2.to(device)\n",
        "            labels    = labels.to(device)\n",
        "            x1_mask   = x1_mask.to(device)\n",
        "            x2_mask   = x2_mask.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # If your model returns (ll_gaussian, ll_vine), you might unify them,\n",
        "            # but for now let's keep your existing logic:\n",
        "            ll_gaussian, ll_vine = model(padded_x1, padded_x2, mask1=x1_mask, mask2=x2_mask)\n",
        "\n",
        "            loss = contrastive_loss(ll_gaussian, ll_vine, labels, alpha=0.3, base_margin=500.0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        epoch_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        ################################################\n",
        "        # 2) VALIDATE AFTER EVERY EPOCH\n",
        "        ################################################\n",
        "        val_metrics = validate(model, val_loader, device)\n",
        "        print(f\"Validation Metrics @ Epoch {epoch+1}: {val_metrics}\")\n",
        "\n",
        "        # Collect metrics\n",
        "        auc_roc_values.append(val_metrics['auc_roc'])\n",
        "        accuracy_values.append(val_metrics['accuracy'])\n",
        "        precision_values.append(val_metrics['precision'])\n",
        "        recall_values.append(val_metrics['recall'])\n",
        "\n",
        "        # Example LR-scheduler usage (comment out if you don't need it)\n",
        "        # scheduler.step(val_metrics['auc_roc'])\n",
        "\n",
        "        ################################################\n",
        "        # 3) PLOT LIKELIHOOD SEPARATION\n",
        "        #    - Once at start: epoch == 0\n",
        "        #    - Then every 2 epochs => (epoch+1)%2==0 => 2,4,6...\n",
        "        ################################################\n",
        "        if epoch == 0:\n",
        "            print(\"\\n>>> Plotting Log-Likelihoods at the start (Epoch 1) <<<\")\n",
        "            print(\"Training set separation:\")\n",
        "            plot_log_likelihoods(model, train_loader, device, dataset_name=\"Train-Epoch1\")\n",
        "            print(\"Validation set separation:\")\n",
        "            plot_log_likelihoods(model, val_loader, device, dataset_name=\"Val-Epoch1\")\n",
        "\n",
        "        elif (epoch + 1) % 2 == 0:\n",
        "            print(f\"\\n>>> Plotting Log-Likelihoods after {epoch+1} epochs <<<\")\n",
        "            print(\"Training set separation:\")\n",
        "            plot_log_likelihoods(model, train_loader, device,\n",
        "                                 dataset_name=f\"Train-Epoch{epoch+1}\")\n",
        "            print(\"Validation set separation:\")\n",
        "            plot_log_likelihoods(model, val_loader, device,\n",
        "                                 dataset_name=f\"Val-Epoch{epoch+1}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted. Saving model...\")\n",
        "\n",
        "# After training, finalize, e.g.:\n",
        "print(\"\\n--- Final Evaluate on Training Set ---\")\n",
        "plot_log_likelihoods(model, train_loader, device, dataset_name=\"Train-Final\")\n",
        "\n",
        "print(\"\\n--- Final Evaluate on Validation Set ---\")\n",
        "plot_log_likelihoods(model, val_loader, device, dataset_name=\"Val-Final\")\n",
        "\n",
        "# Save your model\n",
        "torch.save(model.state_dict(), 'model_final.pth')\n",
        "\n",
        "# Optionally reload, check\n",
        "model_loaded = AnomalyTransformer(input_dim=input_dim,\n",
        "                                  model_dim=model_dim,\n",
        "                                  num_heads=num_heads,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout).to(device)\n",
        "model_loaded.load_state_dict(torch.load('model_final.pth'), strict=True)\n",
        "model_loaded.eval()\n",
        "\n",
        "print(\"\\n--- Reloaded Model: Evaluate on Training ---\")\n",
        "plot_log_likelihoods(model_loaded, train_loader, device, dataset_name=\"Train Reloaded\")\n",
        "\n",
        "print(\"\\n--- Reloaded Model: Evaluate on Validation ---\")\n",
        "plot_log_likelihoods(model_loaded, val_loader, device, dataset_name=\"Val Reloaded\")\n",
        "\n",
        "# If you want to plot your metrics:\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "epochs_array = np.arange(1, num_epochs+1)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs_array, auc_roc_values,    label=\"AUC\",   marker='o')\n",
        "plt.plot(epochs_array, accuracy_values,   label=\"Acc\",   marker='s')\n",
        "plt.plot(epochs_array, precision_values,  label=\"Prec\",  marker='^')\n",
        "plt.plot(epochs_array, recall_values,     label=\"Recall\",marker='x')\n",
        "plt.legend()\n",
        "plt.title(\"Validation metrics per epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbZ4FNWlYuZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example data for precision, recall, and AUC-ROC (replace these with your actual values)\n",
        "epochs = np.arange(1, 26)\n",
        "\n",
        "# Plot the metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, precision_values, label='Precision', color='blue', marker='o', markersize=3, linestyle='-')\n",
        "plt.plot(epochs, recall_values, label='Recall', color='green', marker='s', markersize=3, linestyle='--')\n",
        "plt.plot(epochs, auc_roc_values, label='AUC-ROC', color='red', marker='^', markersize=3, linestyle='-.')\n",
        "\n",
        "# Add titles and labels\n",
        "#plt.title('Performance Metrics Over Epochs', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=18)\n",
        "plt.ylabel('Metric Values', fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "HeRbAFVtwYlm",
        "outputId": "ffc2b602-3d51-41fa-93d9-25dca49222d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PsgULBwIF1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3524cff2-b0d1-4477-9fd3-f1dd07b9ca43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-75698393c95d>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model_gp_copula_likelihood_loss_margin_500_real_anomaly_70_percent_alpha_03.pth'), strict=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# Load the saved state_dict, ignoring missing keys\n",
        "model.load_state_dict(torch.load('model_gp_copula_likelihood_loss_margin_500_real_anomaly_70_percent_alpha_03.pth'), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hEQr7QAQEUCL"
      },
      "outputs": [],
      "source": [
        "# Plot for Training Set\n",
        "plot_log_likelihoods(model, train_loader, device, dataset_name=\"Training\")\n",
        "\n",
        "# Plot for Validation Set\n",
        "#plot_log_likelihoods(model, val_loader, device, dataset_name=\"Validation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ipjKzrSDJCzu"
      },
      "outputs": [],
      "source": [
        "# Plot for Validation Set\n",
        "plot_log_likelihoods(model, val_loader, device, dataset_name=\"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5HuImnxzHXXu"
      },
      "outputs": [],
      "source": [
        "# Plot for Validation Set\n",
        "plot_log_likelihoods(model, test_loader, device, dataset_name=\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsZUvm02Z-CC"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_latent_space(model, loader, device, dataset_name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    This function visualizes the latent space by extracting embeddings from the model\n",
        "    for each batch in 'loader'. We'll handle 3 batch-size scenarios:\n",
        "      1) 5 items: (x1, x2, labels, x1_mask, x2_mask)\n",
        "      2) 3 items: (x1, x2, labels)\n",
        "      3) 2 items: (features, labels)\n",
        "    We'll assume the model has a method 'get_embeddings(x1, x2, mask1, mask2)'\n",
        "    that returns an embedding (e.g. shape (batch_size, embed_dim)).\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import torch\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "    all_labels     = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if len(batch) == 5:\n",
        "                # (padded_x1, padded_x2, batch_labels, x1_mask, x2_mask)\n",
        "                padded_x1, padded_x2, batch_labels, x1_mask, x2_mask = batch\n",
        "\n",
        "                padded_x1 = padded_x1.to(device, dtype=torch.float32)\n",
        "                padded_x2 = padded_x2.to(device, dtype=torch.float32)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                x1_mask = x1_mask.to(device)\n",
        "                x2_mask = x2_mask.to(device)\n",
        "\n",
        "                # We'll call some method get_embeddings() if your model has it\n",
        "                embeddings = model.get_embeddings(padded_x1, padded_x2, mask1=x1_mask, mask2=x2_mask)\n",
        "\n",
        "            elif len(batch) == 3:\n",
        "                # (padded_x1, padded_x2, batch_labels)\n",
        "                padded_x1, padded_x2, batch_labels = batch\n",
        "\n",
        "                padded_x1 = padded_x1.to(device, dtype=torch.float32)\n",
        "                padded_x2 = padded_x2.to(device, dtype=torch.float32)\n",
        "                batch_labels= batch_labels.to(device)\n",
        "\n",
        "                embeddings = model.get_embeddings(padded_x1, padded_x2)\n",
        "\n",
        "            elif len(batch) == 2:\n",
        "                # (features, labels)\n",
        "                features, batch_labels = batch\n",
        "\n",
        "                features = features.to(device, dtype=torch.float32)\n",
        "                batch_labels= batch_labels.to(device)\n",
        "\n",
        "                # If model expects two inputs, pass features twice\n",
        "                # or adapt if your model uses a single input in get_embeddings\n",
        "                embeddings = model.get_embeddings(features, features)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected number of items in batch: {len(batch)}\")\n",
        "\n",
        "            # Convert embeddings + labels to CPU\n",
        "            emb_np = embeddings.cpu().numpy()\n",
        "            lbl_np = batch_labels.cpu().numpy()\n",
        "\n",
        "            all_embeddings.append(emb_np)\n",
        "            all_labels.append(lbl_np)\n",
        "\n",
        "    # Concat all\n",
        "    all_embeddings = np.concatenate(all_embeddings, axis=0)  # shape (total_samples, embed_dim)\n",
        "    all_labels     = np.concatenate(all_labels,     axis=0)  # shape (total_samples,)\n",
        "\n",
        "    # Suppose we do a 2D visualization via e.g. TSNE\n",
        "    from sklearn.manifold import TSNE\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    emb_2d = tsne.fit_transform(all_embeddings)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8,6))\n",
        "    # separate normal vs anomaly for coloring\n",
        "    normal_mask  = (all_labels == 0)\n",
        "    anomaly_mask = (all_labels == 1)\n",
        "\n",
        "    plt.scatter(emb_2d[normal_mask,0],  emb_2d[normal_mask,1],  c='blue',  alpha=0.5, label='Normal')\n",
        "    plt.scatter(emb_2d[anomaly_mask,0], emb_2d[anomaly_mask,1], c='red',   alpha=0.5, label='Anomaly')\n",
        "\n",
        "    plt.title(f\"Latent Space Visualization - {dataset_name}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5oVL0eGJm8QT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgeSHYbfFz36",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Visualize the latent space for the training set\n",
        "#visualize_latent_space(model, train_loader, device, dataset_name=\"Train\")\n",
        "\n",
        "# Visualize the latent space for the test set\n",
        "visualize_latent_space(model, val_loader, device, dataset_name=\"Validation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPvuqbvF9fIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vi8ID8G99fMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbQ6ZiRB9fQc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}